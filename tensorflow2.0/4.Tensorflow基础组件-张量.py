import tensorflow as tf
import numpy as np

# å¼ é‡æ˜¯å…·æœ‰ç»Ÿä¸€ç±»å‹ï¼ˆç§°ä¸º dtypeï¼‰çš„å¤šç»´æ•°ç»„ã€‚æ‚¨å¯ä»¥åœ¨ tf.dtypes.DType ä¸­æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„ dtypesã€‚
# å¦‚æœæ‚¨ç†Ÿæ‚‰ NumPyï¼Œå°±ä¼šçŸ¥é“å¼ é‡ä¸ np.arrays æœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ã€‚
# å°±åƒ Python æ•°å€¼å’Œå­—ç¬¦ä¸²ä¸€æ ·ï¼Œæ‰€æœ‰å¼ é‡éƒ½æ˜¯ä¸å¯å˜çš„ï¼šæ°¸è¿œæ— æ³•æ›´æ–°å¼ é‡çš„å†…å®¹ï¼Œåªèƒ½åˆ›å»ºæ–°çš„å¼ é‡ã€‚

#%% 1 åŸºç¡€çŸ¥è¯†

# æˆ‘ä»¬æ¥åˆ›å»ºä¸€äº›åŸºæœ¬å¼ é‡ã€‚
# ä¸‹é¢æ˜¯ä¸€ä¸ªâ€œæ ‡é‡â€ï¼ˆæˆ–ç§°â€œ0 ç§©â€å¼ é‡ï¼‰ã€‚æ ‡é‡åŒ…å«å•ä¸ªå€¼ï¼Œä½†æ²¡æœ‰â€œè½´â€ã€‚
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
# â€œå‘é‡â€ï¼ˆæˆ–ç§°â€œ1 ç§©â€å¼ é‡ï¼‰å°±åƒä¸€ä¸ªå€¼çš„åˆ—è¡¨ã€‚å‘é‡æœ‰ 1 ä¸ªè½´ï¼š
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
# â€œçŸ©é˜µâ€ï¼ˆæˆ–ç§°â€œ2 ç§©â€å¼ é‡ï¼‰æœ‰ 2 ä¸ªè½´ï¼š
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
# å¼ é‡çš„è½´å¯èƒ½æ›´å¤šï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªåŒ…å« 3 ä¸ªè½´çš„å¼ é‡ï¼š
rank_3_tensor = tf.constant([
    [[0, 1, 2, 3, 4],
     [5, 6, 7, 8, 9]],
    [[10, 11, 12, 13, 14],
     [15, 16, 17, 18, 19]],
    [[20, 21, 22, 23, 24],
     [25, 26, 27, 28, 29]], ])

print(rank_3_tensor)

# é€šè¿‡ä½¿ç”¨ np.array æˆ– tensor.numpy æ–¹æ³•ï¼Œæ‚¨å¯ä»¥å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼š
np.array(rank_2_tensor)
rank_2_tensor.numpy()

# å¼ é‡é€šå¸¸åŒ…å«æµ®ç‚¹å‹å’Œæ•´å‹æ•°æ®ï¼Œä½†æ˜¯è¿˜æœ‰è®¸å¤šå…¶ä»–æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬ï¼šå¤æ‚çš„æ•°å€¼/å­—ç¬¦ä¸²
# tf.Tensor åŸºç±»è¦æ±‚å¼ é‡æ˜¯â€œçŸ©å½¢â€â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªè½´ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ å¤§å°ç›¸åŒã€‚ä½†æ˜¯ï¼Œå¼ é‡æœ‰å¯ä»¥å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹æ®Šç±»å‹ã€‚
# ä¸è§„åˆ™å¼ é‡ï¼ˆå‚é˜…ä¸‹æ–‡ä¸­çš„ RaggedTensorï¼‰
# ç¨€ç–å¼ é‡ï¼ˆå‚é˜…ä¸‹æ–‡ä¸­çš„ SparseTensorï¼‰
# æˆ‘ä»¬å¯ä»¥å¯¹å¼ é‡æ‰§è¡ŒåŸºæœ¬æ•°å­¦è¿ç®—ï¼ŒåŒ…æ‹¬åŠ æ³•ã€é€å…ƒç´ ä¹˜æ³•å’ŒçŸ©é˜µä¹˜æ³•è¿ç®—ã€‚
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # Could have also said `tf.ones([2,2])`

print(tf.add(a, b), "\n")
print(tf.multiply(a, b), "\n")
print(tf.matmul(a, b), "\n")
# å„ç§è¿ç®— (op) éƒ½å¯ä»¥ä½¿ç”¨å¼ é‡ã€‚
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])
# Find the largest value
print(tf.reduce_max(c))
# Find the index of the largest value
print(tf.argmax(c))
# Compute the softmax
print(tf.nn.softmax(c))
#%% 2 å½¢çŠ¶ç®€ä»‹
# å¼ é‡æœ‰å½¢çŠ¶ã€‚ä¸‹é¢æ˜¯å‡ ä¸ªç›¸å…³æœ¯è¯­ï¼šï¼ˆå¼ é‡å’Œ tf.TensorShape å¯¹è±¡æä¾›äº†æ–¹ä¾¿çš„å±æ€§æ¥è®¿é—®ï¼šï¼‰
# å½¢çŠ¶ï¼šå¼ é‡çš„æ¯ä¸ªç»´åº¦çš„é•¿åº¦ï¼ˆå…ƒç´ æ•°é‡ï¼‰ã€‚
# ç§©ï¼šå¼ é‡çš„ç»´åº¦æ•°é‡ã€‚æ ‡é‡çš„ç§©ä¸º 0ï¼Œå‘é‡çš„ç§©ä¸º 1ï¼ŒçŸ©é˜µçš„ç§©ä¸º 2ã€‚
# è½´æˆ–ç»´åº¦ï¼šå¼ é‡çš„ä¸€ä¸ªç‰¹æ®Šç»´åº¦ã€‚
# å¤§å°ï¼šå¼ é‡çš„æ€»é¡¹æ•°ï¼Œå³ä¹˜ç§¯å½¢çŠ¶å‘é‡
rank_4_tensor = tf.zeros([3, 2, 4, 5])
# 4ç§©å¼ é‡ï¼Œå½¢çŠ¶ï¼š[3ï¼Œ2ï¼Œ4ï¼Œ5]
print("Type of every element:", rank_4_tensor.dtype)
print("Number of dimensions:", rank_4_tensor.ndim)
print("Shape of tensor:", rank_4_tensor.shape)
print("Elements along axis 0 of tensor:", rank_4_tensor.shape[0])
print("Elements along the last axis of tensor:", rank_4_tensor.shape[-1])
print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy())

# è™½ç„¶é€šå¸¸ç”¨ç´¢å¼•æ¥æŒ‡ä»£è½´ï¼Œä½†æ˜¯æ‚¨å§‹ç»ˆè¦è®°ä½æ¯ä¸ªè½´çš„å«ä¹‰ã€‚è½´ä¸€èˆ¬æŒ‰ç…§ä»å…¨å±€åˆ°å±€éƒ¨çš„é¡ºåºè¿›è¡Œæ’åºï¼š
# é¦–å…ˆæ˜¯æ‰¹æ¬¡è½´ï¼Œéšåæ˜¯ç©ºé—´ç»´åº¦ï¼Œæœ€åæ˜¯æ¯ä¸ªä½ç½®çš„ç‰¹å¾ã€‚è¿™æ ·ï¼Œåœ¨å†…å­˜ä¸­ï¼Œç‰¹å¾å‘é‡å°±ä¼šä½äºè¿ç»­çš„åŒºåŸŸã€‚
#%% 3 ç´¢å¼•
# å•è½´ç´¢å¼•
# TensorFlow éµå¾ªæ ‡å‡† Python ç´¢å¼•è§„åˆ™ï¼ˆç±»ä¼¼äºåœ¨ Python ä¸­ä¸ºåˆ—è¡¨æˆ–å­—ç¬¦ä¸²ç¼–åˆ¶ç´¢å¼•ï¼‰ä»¥åŠ NumPy ç´¢å¼•çš„åŸºæœ¬è§„åˆ™ã€‚
# ç´¢å¼•ä» 0 å¼€å§‹ç¼–åˆ¶/è´Ÿç´¢å¼•è¡¨ç¤ºæŒ‰å€’åºç¼–åˆ¶ç´¢å¼•/å†’å· : ç”¨äºåˆ‡ç‰‡ start:stop:step
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
# ä½¿ç”¨æ ‡é‡ç¼–åˆ¶ç´¢å¼•ä¼šç§»é™¤ç»´åº¦ï¼š
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
# ä½¿ç”¨ : åˆ‡ç‰‡ç¼–åˆ¶ç´¢å¼•ä¼šä¿ç•™ç»´åº¦ï¼š
print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())

# å¤šè½´ç´¢å¼•
# æ›´é«˜ç§©çš„å¼ é‡é€šè¿‡ä¼ é€’å¤šä¸ªç´¢å¼•æ¥ç¼–åˆ¶ç´¢å¼•ã€‚
# å¯¹äºé«˜ç§©å¼ é‡çš„æ¯ä¸ªå•ç‹¬çš„è½´ï¼Œéµå¾ªä¸å•è½´æƒ…å½¢å®Œå…¨ç›¸åŒçš„ç´¢å¼•è§„åˆ™ã€‚
print(rank_2_tensor.numpy())
# ä¸ºæ¯ä¸ªç´¢å¼•ä¼ é€’ä¸€ä¸ªæ•´æ•°ï¼Œç»“æœæ˜¯ä¸€ä¸ªæ ‡é‡ã€‚
print(rank_2_tensor[1, 1].numpy())
# æ‚¨å¯ä»¥ä½¿ç”¨æ•´æ•°ä¸åˆ‡ç‰‡çš„ä»»æ„ç»„åˆç¼–åˆ¶ç´¢å¼•ï¼š
# Get row and column tensors
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")

#%% 4 æ“ä½œå½¢çŠ¶
# Reshaping a tensor is of great utility.
var_x = tf.Variable(tf.constant([[1], [2], [3]]))
print(var_x.shape)
# You can convert this object into a Python list, too
print(var_x.shape.as_list())
# é€šè¿‡é‡æ„å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ã€‚é‡æ„çš„é€Ÿåº¦å¾ˆå¿«ï¼Œèµ„æºæ¶ˆè€—å¾ˆä½ï¼Œå› ä¸ºä¸éœ€è¦å¤åˆ¶åº•å±‚æ•°æ®ã€‚
reshaped = tf.reshape(var_x,[1,3])
print(var_x.shape)
print(reshaped.shape)
# æ•°æ®åœ¨å†…å­˜ä¸­çš„å¸ƒå±€ä¿æŒä¸å˜ï¼ŒåŒæ—¶ä½¿ç”¨è¯·æ±‚çš„å½¢çŠ¶åˆ›å»ºä¸€ä¸ªæŒ‡å‘åŒä¸€æ•°æ®çš„æ–°å¼ é‡ã€‚
# TensorFlow é‡‡ç”¨ C æ ·å¼çš„â€œè¡Œä¼˜å…ˆâ€å†…å­˜è®¿é—®é¡ºåºï¼Œå³æœ€å³ä¾§çš„ç´¢å¼•å€¼é€’å¢å¯¹åº”äºå†…å­˜ä¸­çš„å•æ­¥ä½ç§»ã€‚
# å¦‚æœæ‚¨å±•å¹³å¼ é‡ï¼Œåˆ™å¯ä»¥çœ‹åˆ°å®ƒåœ¨å†…å­˜ä¸­çš„æ’åˆ—é¡ºåºã€‚
# A `-1` passed in the `shape` argument says "Whatever fits".
print(tf.reshape(rank_3_tensor, [-1]))

# ä¸€èˆ¬æ¥è¯´ï¼Œtf.reshape å”¯ä¸€åˆç†çš„ç”¨é€”æ˜¯ç”¨äºåˆå¹¶æˆ–æ‹†åˆ†ç›¸é‚»è½´ï¼ˆæˆ–æ·»åŠ /ç§»é™¤ 1ï¼‰ã€‚
# å¯¹äº 3x2x5 å¼ é‡ï¼Œé‡æ„ä¸º (3x2)x5 æˆ– 3x(2x5) éƒ½åˆç†ï¼Œå› ä¸ºåˆ‡ç‰‡ä¸ä¼šæ··æ·†ï¼š
print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))

# é‡æ„å¯ä»¥å¤„ç†æ€»å…ƒç´ ä¸ªæ•°ç›¸åŒçš„ä»»ä½•æ–°å½¢çŠ¶ï¼Œä½†æ˜¯å¦‚æœä¸éµä»è½´çš„é¡ºåºï¼Œåˆ™ä¸ä¼šå‘æŒ¥ä»»ä½•ä½œç”¨ã€‚
# åˆ©ç”¨ tf.reshape æ— æ³•å®ç°è½´çš„äº¤æ¢ï¼Œè¦äº¤æ¢è½´ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ tf.transposeã€‚
# You can't reorder axes with reshape.
print(tf.reshape(rank_3_tensor, [2, 3, 5]), "\n")
# This is a mess
print(tf.reshape(rank_3_tensor, [5, 6]), "\n")
# tf.transpose
print(tf.transpose(rank_3_tensor), "\n")

# æ‚¨å¯èƒ½ä¼šé‡åˆ°éå®Œå…¨æŒ‡å®šçš„å½¢çŠ¶ã€‚è¦ä¹ˆæ˜¯å½¢çŠ¶åŒ…å« None ç»´åº¦ï¼ˆç»´åº¦çš„é•¿åº¦æœªçŸ¥ï¼‰ï¼Œè¦ä¹ˆæ˜¯å½¢çŠ¶ä¸º Noneï¼ˆå¼ é‡çš„ç§©æœªçŸ¥ï¼‰ã€‚
# é™¤äº† tf.RaggedTensor å¤–ï¼Œè¿™ç§æƒ…å†µåªä¼šåœ¨ TensorFlow çš„ç¬¦å·åŒ–è®¡ç®—å›¾æ„å»º API ç¯å¢ƒä¸­å‡ºç°ï¼štf.function/keras

#%% 5 Dtypesè¯¦è§£
# ä½¿ç”¨ Tensor.dtype å±æ€§å¯ä»¥æ£€æŸ¥ tf.Tensor çš„æ•°æ®ç±»å‹ã€‚
# ä» Python å¯¹è±¡åˆ›å»º tf.Tensor æ—¶ï¼Œæ‚¨å¯ä»¥é€‰æ‹©æŒ‡å®šæ•°æ®ç±»å‹ã€‚

# å¦‚æœä¸æŒ‡å®šï¼ŒTensorFlow ä¼šé€‰æ‹©ä¸€ä¸ªå¯ä»¥è¡¨ç¤ºæ‚¨çš„æ•°æ®çš„æ•°æ®ç±»å‹ã€‚TensorFlow å°† Python æ•´æ•°è½¬æ¢ä¸º tf.int32ï¼Œå°† Python æµ®ç‚¹æ•°è½¬æ¢ä¸º tf.float32ã€‚
# å¦å¤–ï¼Œå½“è½¬æ¢ä¸ºæ•°ç»„æ—¶ï¼ŒTensorFlow ä¼šé‡‡ç”¨ä¸ NumPy ç›¸åŒçš„è§„åˆ™ã€‚
# ä½¿ç”¨castå°†æ•°æ®ç±»å‹è½¬æ¢
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# Now, let's cast to an uint8 and lose the decimal precision
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)

#%% 6 å¹¿æ’­
# å¹¿æ’­æ˜¯ä» NumPy ä¸­çš„ç­‰æ•ˆåŠŸèƒ½å€Ÿç”¨çš„ä¸€ä¸ªæ¦‚å¿µã€‚ç®€è€Œè¨€ä¹‹ï¼Œåœ¨ä¸€å®šæ¡ä»¶ä¸‹ï¼Œå¯¹ä¸€ç»„å¼ é‡æ‰§è¡Œç»„åˆè¿ç®—æ—¶ï¼Œä¸ºäº†é€‚åº”å¤§å¼ é‡ï¼Œä¼šå¯¹å°å¼ é‡è¿›è¡Œâ€œæ‰©å±•â€ã€‚
x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])
# All of these are the same computation
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
# åŒæ ·ï¼Œå¯ä»¥æ‰©å±•å¤§å°ä¸º 1 çš„ç»´åº¦ï¼Œä½¿å…¶ç¬¦åˆå…¶ä»–å‚æ•°ã€‚åœ¨åŒä¸€ä¸ªè®¡ç®—ä¸­å¯ä»¥åŒæ—¶æ‰©å±•ä¸¤ä¸ªå‚æ•°ã€‚
x = tf.reshape(x,[3,1])
y = tf.range(1, 5)
print(x, "\n")
print(y, "\n")
# å’ŒçŸ©é˜µè¿ç®—ä¸€æ ·ï¼Œè¡Œä¹˜åˆ—
print(tf.multiply(x, y))
# ä¸‹é¢æ˜¯ä¸ä½¿ç”¨å¹¿æ’­çš„åŒä¸€è¿ç®—ï¼š
x_stretch = tf.constant([[1, 1, 1, 1],
                         [2, 2, 2, 2],
                         [3, 3, 3, 3]])

y_stretch = tf.constant([[1, 2, 3, 4],
                         [1, 2, 3, 4],
                         [1, 2, 3, 4]])

print(x_stretch * y_stretch)  # Again, operator overloading
# åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¹¿æ’­çš„æ—¶é—´å’Œç©ºé—´æ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºå¹¿æ’­è¿ç®—ä¸ä¼šåœ¨å†…å­˜ä¸­å…·ä½“åŒ–æ‰©å±•çš„å¼ é‡ã€‚
# ä½¿ç”¨ tf.broadcast_to å¯ä»¥äº†è§£å¹¿æ’­çš„è¿ç®—æ–¹å¼ã€‚
print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))

#%% 7 tf.convert_to_tensor è½¬åŒ–ä¸ºå¼ é‡
# å¤§éƒ¨åˆ†è¿ç®—ï¼ˆå¦‚ tf.matmul å’Œ tf.reshapeï¼‰ä¼šä½¿ç”¨ tf.Tensor ç±»çš„å‚æ•°ã€‚ä¸è¿‡ï¼Œåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨ä¼šå‘ç°æˆ‘ä»¬ç»å¸¸ä¼ é€’å½¢çŠ¶ç±»ä¼¼äºå¼ é‡çš„ Python å¯¹è±¡
# å¤§éƒ¨åˆ†ï¼ˆä½†å¹¶éå…¨éƒ¨ï¼‰è¿ç®—ä¼šåœ¨éå¼ é‡å‚æ•°ä¸Šè°ƒç”¨ convert_to_tensorã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè½¬æ¢æ³¨å†Œè¡¨ï¼Œå¤§å¤šæ•°å¯¹è±¡ç±»
# ï¼ˆå¦‚ NumPy çš„ ndarrayã€TensorShapeã€Python åˆ—è¡¨å’Œ tf.Variableï¼‰éƒ½å¯ä»¥è‡ªåŠ¨è½¬æ¢ã€‚
# æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… tf.register_tensor_conversion_functionã€‚å¦‚æœæ‚¨æœ‰è‡ªå·±çš„ç±»å‹ï¼Œåˆ™å¯èƒ½å¸Œæœ›è‡ªåŠ¨è½¬æ¢ä¸ºå¼ é‡ã€‚ # å³æ”¯æŒç¬¬ä¸‰æ–¹ç±»å‹è½¬æ¢ä¸ºå¼ é‡


#%% 8 ä¸è§„åˆ™å¼ é‡
# å¦‚æœå¼ é‡çš„æŸä¸ªè½´ä¸Šçš„å…ƒç´ ä¸ªæ•°å¯å˜ï¼Œåˆ™ç§°ä¸ºâ€œä¸è§„åˆ™â€å¼ é‡ã€‚å¯¹äºä¸è§„åˆ™æ•°æ®ï¼Œè¯·ä½¿ç”¨ tf.ragged.RaggedTensorã€‚
# ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä¾‹å­æ— æ³•ç”¨è§„åˆ™å¼ é‡è¡¨ç¤ºï¼š
"""
0 1 2 3
4 5 
6 7 8
9
"""
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f"{type(e).__name__}: {e}")

# åº”ä½¿ç”¨ tf.ragged.constant æ¥åˆ›å»º tf.RaggedTensorï¼š
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
# tf.RaggedTensor çš„å½¢çŠ¶åŒ…å«æœªçŸ¥ç»´åº¦ï¼š
print(ragged_tensor.shape)

#%% 9 å­—ç¬¦ä¸²å¼ é‡
# tf.string æ˜¯ä¸€ç§ dtypeï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å¼ é‡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å­—ç¬¦ä¸²ï¼ˆå¯å˜é•¿åº¦å­—èŠ‚æ•°ç»„ï¼‰æ¥è¡¨ç¤ºæ•°æ®ã€‚
# å­—ç¬¦ä¸²æ˜¯åŸå­ç±»å‹ï¼Œæ— æ³•åƒ Python å­—ç¬¦ä¸²ä¸€æ ·ç¼–åˆ¶ç´¢å¼•ã€‚å­—ç¬¦ä¸²çš„é•¿åº¦å¹¶ä¸æ˜¯å¼ é‡çš„ä¸€ä¸ªç»´åº¦ã€‚æœ‰å…³æ“ä½œå­—ç¬¦ä¸²çš„å‡½æ•°ï¼Œè¯·å‚é˜… tf.stringsã€‚
# Tensors can be strings, too here is a scalar string.
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)
print(scalar_string_tensor.shape)
# å¦‚æœä¼ é€’ Unicode å­—ç¬¦ï¼Œåˆ™ä¼šä½¿ç”¨ utf-8 ç¼–ç ã€‚
tf.constant("ğŸ¥³ğŸ‘")
# åœ¨ tf.strings ä¸­å¯ä»¥æ‰¾åˆ°ç”¨äºæ“ä½œå­—ç¬¦ä¸²çš„ä¸€äº›åŸºæœ¬å‡½æ•°ï¼ŒåŒ…æ‹¬ tf.strings.splitã€‚
print(tf.strings.split(scalar_string_tensor, sep=" "))

# ...but it turns into a `RaggedTensor` if we split up a tensor of strings,
# as each string might be split into a different number of parts.
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
# Note that the shape is (3,). The string length is not included.
print(tensor_of_strings)
print(tf.strings.split(tensor_of_strings))

# tf.string.to_numberï¼š
text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))
# è™½ç„¶ä¸èƒ½ä½¿ç”¨ tf.cast å°†å­—ç¬¦ä¸²å¼ é‡è½¬æ¢ä¸ºæ•°å€¼ï¼Œä½†æ˜¯å¯ä»¥å…ˆå°†å…¶è½¬æ¢ä¸ºå­—èŠ‚ï¼Œç„¶åè½¬æ¢ä¸ºæ•°å€¼ã€‚
byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)

# Or split it up as unicode and then decode it
unicode_bytes = tf.constant("ã‚¢ãƒ’ãƒ« ğŸ¦†")
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, "UTF-8")
unicode_values = tf.strings.unicode_decode(unicode_bytes, "UTF-8")

print("\nUnicode bytes:", unicode_bytes)
print("\nUnicode chars:", unicode_char_bytes)
print("\nUnicode values:", unicode_values)

# tf.io æ¨¡å—åŒ…å«åœ¨æ•°æ®ä¸å­—èŠ‚ç±»å‹ä¹‹é—´è¿›è¡Œç›¸äº’è½¬æ¢çš„å‡½æ•°ï¼ŒåŒ…æ‹¬è§£ç å›¾åƒå’Œè§£æ csv çš„å‡½æ•°


#%% 10 ç¨€ç–å¼ é‡
# åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ•°æ®å¾ˆç¨€ç–ï¼Œæ¯”å¦‚è¯´åœ¨ä¸€ä¸ªéå¸¸å®½çš„åµŒå…¥ç©ºé—´ä¸­ã€‚ä¸ºäº†é«˜æ•ˆå­˜å‚¨ç¨€ç–æ•°æ®ï¼ŒTensorFlow æ”¯æŒ tf.sparse.SparseTensor å’Œç›¸å…³è¿ç®—ã€‚
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")

# We can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))
